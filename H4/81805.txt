Като начало свалих tarball от kernel.org със стабилна версия. Избрах най-новата стабилна версия
5.9.3, защото има най-много feature-и, които мога да разгледам. Разбира се искам минимален kernel,
но все пак може да е интересно какви възможности имат най-новите linux kernel-и. Като видях
изискването за 15-20гб на машината ми леко се притесних, защото в момента имам 34гб свободни :(.
После ще трябва да почистя всички файлове да си ги върна, че всеки гигабайт е ценен XD. След като
свалих kernel-а и го разархивирах прочетох наново презентацията да си припомня за какво сме
говорили. После седнах да видя каква беше разликата между make menuconfig и make oldconfig. Другите
опции си спомням, че не бяха препоръчителни, защото са неудобни за работа. Намерих този сайт
http://embeddedguruji.blogspot.com/2019/01/make-config-vs-oldconfig-vs-defconfig.html И разбрах, че
make oldconfig е полезно при обновяване на версията на kernel-а и пита само за нови feature-и дали
искам да ги пусна. 

Другото, което исках да разбера е каква точно беше разликата между make clean, make mrproper и make
distclean. По спомен make clean не триеше конфигурационни файлове за разлика от другите 2, ама
разликата между mrpopre и distclean ми се губи. При търсенето си намерих този линк:
https://www.linuxtopia.org/online_books/linux_kernel/kernel_configuration/ch11s02.html От него
разбрах, че clean < mrproper < distclean. Като distclean трие някои backup файлове, които mrpoper не
трие.  Намерих и този линк в stack overflow, който уточни точно какви са тези допълнителни файлове,
които distclean трие:
https://unix.stackexchange.com/questions/550483/make-mrproper-vs-make-distclean-in-the-linux-kernel
Не знам на 100% всичките типове файлове за какво са, но TAG-ове съм използвал и преди под формата на
CTAGS за прескачане в проекти, orig bak и тн са очевиндо backup-и генерирани при някои команди
(vimdiff генерира orig примерно, макар че надали make използва конкретно него).

Като следваща стъпка реших да направя make menuconfig и като влезнах първата ми работа беше да
прочета help менюто преди да започна да разглеждам всичките опции. Да натискам на посоки също може
да доведе до това да направя и някоя глупост особено, както нямам никакъв контекст. Разбрах, че и
hjkl работят за scroll-ване в menuconfig, което много го оценявам :D.

tldr за мене си: [] - може да е built in или removed <> - built in, module или removed, {} - built
in или module, "- -" -избран от друг feature. * - build in, M - module, whitespace - exclude.
shortcuts: Z - hidden files. q - за exit

От help страницата горната информация ми се стори най-важна. Цялостно имаше и немалко други неща,
ама реших за лесно да си отделя тези. Реално много лесно мога да отворя help-а пак и да ги видя, ама
здраве да е. Да покажа, че реално чета поне, а не, че само се преструвам тука :D. Друго мега важно
нещо беше, че -----> значи подменю. Така мога да различавам опции и менюта предполагам и да не ме е
страх, че ако натисна нещо ще се счупи всичко. 

Идеята ми за build-ване на kernel е да направя нещо работещо. Като гледам има бая опции и ако някоя
опция ми се струва нужна ще я оставя. Ако не съм сигурен, пак ще я оставя и после ще rebuild-на
kernel-а без нея, ако не съм убеден дали ще работи.  Примерно видях, че SysV IPC може да бъде
премахнато. В момента чета защо е SysV, но като цяло би имало софтуер, който няма да работи без този
feature. Затова мисля да го оставя към момента, да получа нещо работещо. После смятам да го спра и
да видя дали всичко ще се счупи. Щом може да се спре очаквам ОС-а да тръгне на теоритична основа.
Все пак реших да погледна и този линк за повече информация:
https://tldp.org/LDP/lpg/node21.html#SECTION00740000000000000000 . Както си мислех това са точно
семафори, message queue-та и shared memory сегменти. Това не знам дали е нужно да тръгне
операционната система, но със сигурност е важно, да работят приложенията на нея като хората и затова
ще го оставя.

Прочетох writev and readv man page-а и какво е watch queue:
https://www.kernel.org/doc/Documentation/watch_queue.rst . Като осъзнах, че writev е различно от
process_vm_writev. После прочетох другия man page, в който е обяснено, че може един процес с тази
команда да пише в паметта на друг. Това е много интересен и на пръв поглед опасен feature. Реших, че
е ненужен в общия случай, защото не ми изглежда удачно две програми да си пишат в паметта, но
вероятно има смисъл от него.

Разбрах за разликите между libc5 и glibc и историята зад тях. Спрях libc5 syscall-овете, защото вече
не се използват така или иначе. 

Прочетох за timer interrupt-ите и в кои случаи бихме искали да са на период(доколкото разбрах никога
почти), да са само когато машината не е idle(почти винаги) и само когато машината има работа или
само един процес работи(ако има един процес, който ако го паузираме би забавил цялата система
примерно):https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt Доколкото разбрах това са
interrupt-ите, които служат за смяна между процеси. Ако искаме да имаме процесор, който да
multitask-ва и да работи с множество прорами най-добре е dyntic-idle режима. Иначе ако имаме само
един процес, който искаме главно той да работи и точи много ресурси е adaptive-tick режима(третия).
Понеже не мисля да имам само един процес го оставям на idle.

Още по темата за таймери прочетох какво са high resoulution timers:
https://elinux.org/High_Resolution_Timers и реших, че за минимален image не ни трябва честотоа
по-бърза от 1 jiffy за таймери.

Използвах и това за повече информация за таймери: https://lwn.net/Articles/115554/

Прекарах няколко часа да чета за таймери и затова толкова подробно ги описвам. Ще се постарая за
другите опции да не съм толкова подробен, че тук реално съм прегледал съвсем малко XD. То реално с
това домашно, което е главно четене не знам какво точно да обясня. За неща като timers не мисля, че
има значение кое ще избера за минимален kernel или поне е малко, така че просто разглеждам за какво
са. Не знам и кои флагове биха били важни и кои не, така че да не пиша твърде много.

Прочетох малко за preemption, всякакви типове логове ще ги спра за финалния build на ядрото. Понеже
очаквам да имам проблеми обаче реших засега да ги държа пуснати, така че да получа информация какво
се чупи. Така на теорият ядрото ще е ултра минимално. За нормална среда това обаче е много лоша
идея. Даже може това да е аргумент защо е по-добре да не ги спираш. Прегледах man (2,5) acct също за
BSD Process accounting. Спрях всички логове за процеси, които може да вземем от kernel-а. 

Стана ми интересно какво е cpu pressure и прочетох това:
https://github.com/torvalds/linux/blob/master/Documentation/accounting/psi.rst PSI показва колко
време се чака за даден ресурс за извършване на задачи. Следи в реално време колко ресурс се използва
в дадена машина и помага при преценка дали ни трябва още.

което ме доведе до това да разуча какво е OOM(Out of memory) killer:
https://rakeshjain-devops.medium.com/linux-out-of-memory-killer-31e477a45759#:~:text=The%20%E2%80%9COOM%20Killer%E2%80%9D%20or%20%E2%80%9C,to%20allocate%20to%20other%20processes.
Убива процесите заемащи най-много памет, като паметта свърши.


За цел краткост няма да се спирам по-подробно на горните две. Погледнах разликата между kthreads и
pthreads тук: https://stackoverflow.com/questions/27581747/pthread-vs-kthread-in-linux-kernel-v2-6 .
Знам, че е stack overflow, ама мисля, че отоговрът е достатъчно добър, а и в момента няма смисъл да
се заравям в подробности.  Ако остане време ще задълбая повече.

Погледнах summary-то на този линк какво е RCU, защото го видях на няколко места, като термин:
https://lwn.net/Articles/262464/ . Основното, което разбрах е, че за дадени обекти държи няколко
версии, докато не минат всички критични операции за чете не минат успешно дори при insertion или
deletion по време на четене. Ще е добре да допрочет данните на страницата, ако остане време.

Видях ePBF споменато на едно място и проверих какво е. Излезна ми това:
https://lwn.net/Articles/740157/ Доколкото разбрах са някакви системни извиквания, които могат да се
използват за филтриране на мрежови пакети, писане на програми за мрежата и дебъгване на kernel-а.

Прочетох и какво е NMI: https://medium.com/@yildirimabdrhm/nmi-watchdog-on-linux-ae3b4c86e8d8 .
Разбрах, че е система, която следи interrupt-ите на компютъра и ако за дълъг период от време не
засече такива счита системата за забила и пуска kernel panic, което докотлкото разбрах от уикипедия
е подобно на blue screen of death, но за линукс.

Това си го записвам, защото ми беше интересно, че мога да кажа за всеки процесор какъв му е
максималния utilization и така scheduler-а може да вземе по-добре, макар CONFIG_UCLAMP_TASK се
казва. Разбира се не го пускам за минималност, но е любопитно.

cgroups знам, че се използват за ограничавне на ресурсите на даден процес. Доколкото съм наясно са
ключови за правене на контейнери и манипулиране на ресурсите, които им се дават. Като цяло искаме
минимален кърнъл и ограничавнето на количеството рам(примерно) за даден процес или група процеси не
ми изглежда ключово и затова ще го изключа. Като гледам дори при build-а на кърнъла са обвързани с
namepsace support, което също се използва за ограничение на това, което виждат контейнерите. Реших
малко да си преговоря и прочетох уики страницата: https://en.wikipedia.org/wiki/Cgroups

Прочетох малко и за CFS: https://en.wikipedia.org/wiki/Completely_Fair_Scheduler Мисля, че преди го
бях чувал, но не знаех точно как работи. Самите червено черни дървета трябва да го проуча, но освен
това разбрах идеята, че всеки процес се слага в дървото на база времето, което е пракарал да работи.
Тези, които повече спат имат по-голям приоритет, така че като им трябва време да го получат. Иначе
за execution се използва най-лявото листо и то после пак се вкарва в дървото на O(logn) време или се
изхвърля, ако няма повече работа.

Прочетох и за RDMA: https://en.wikipedia.org/wiki/Remote_direct_memory_access Директно пишеш в
паметта на приложението като получаваш трафик по мрежата. Трафика не се складира в буфире и копира
от процесора.

intird-то ще го оставя. Вероятно ще ми трябва, да ми тръгне операционната система.

Погледнах оптимизационнит флагове за компилатора. Избирма -0s, защото макар и бавно ще доведе до
по-оптимален/по-малък размер https://gcc.gnu.org/onlinedocs/gnat_ugn/Optimization-Levels.html

Прочетох какво е io_uring: https://lwn.net/Articles/810414/ . По мое разбиране се използва за
асинхронни I/O операции. Старата система не е била толкова добра и това се опитва да я надгради.
Доста често виждах термина: kernel ring buffer включително и в горния линк, но не ми беше съвсем
ясно точно какво е. Прочетох, че kernel логовете се пишат от там и се четат с dmesg командата, която
разгледахме в час. Явно термина съм пропуснал или съм се заблял като сме го говорили.  Иначе
доколкото разбрах io_uring има такъв kernel ring buffer, в които пише команди за I/O, които kernel-а
изпълнява. Понеже е за асинхронно I/O и е ново го спрях.

Прочетох и за futex, макар че не видях разлика със стандартните mutex механизми, които учихме по ОС.
Прилича ми много на семафор: https://linux.die.net/man/7/futex

SLUB: https://lwn.net/Articles/229984/ Стана ми интересно какво е и да си призня спецификите на
имплемнтацията не ги разбирам съвсем. Разбрах главно, че е алтернативен memory manager, които цели
да намали метаданните, които стандартно се използват, които растат експоненциално на база брой
процесори. За memory allocation избрах SLOB с цел минималност така или иначе. Тук много мога да
задълбая вероятно за това как работи SLAB по-добре и как точно имплементацията на SLUB го подобрява,
но това за в бъдеще или ако имам време.  Видях, че SLOB е оптимизирано за embedded и вероятно ще
имам доста проблеми като фрагментация, бавно алокиране и други:
https://nitingupta.dev/post/slob-memory-allocator/ . Все пак реших за целта на минимален кърнъл да
го оставя така.  Не знам дали искаме минимален, но бърз kernel обаче. Тогава вероятно някои условия
бих ги променил. Към момента ми е цел минимален размер. Скоростта вероятно ще е много зле XD.

Разбрах за profiling, като алтернатива на benchmark-ове:
http://www.pixelbeat.org/programming/profiling/#:~:text=Linux%20Profiling%20tools%20and%20techniques,minimising%20external%20influences%20from%20consideration.

Махнах мулти-процесор support.

Научих какво е SoC: https://en.wikipedia.org/wiki/System_on_a_chip . Вместо да имаме отделно
компоненти и чипове за видео карта, процесор и подобни. Всичко е в един чип.

Като търсих IOSF на интел попаднах на това: https://www.otter.org/Public/ и ми се стори сладко и
забавно, така че го добавих тук.

Седнах да прочета какво беше DMI. Видях, че свързва north и south bridge в motherboard-а. Това по
принцип знам, че са важни термини и затова си ги припомних. Видях, че north bridge-а се връзва
директно към процесора и той работи с RAM паметта и pci express, което се използва за работа с неща
като видео карти, звукови карти и подобни. Southbridge-а е вързан към north и служи за по-бавните
I/O операции по-бавните PCI PCIe bus-ове, NVMe storage, USB, BIOS и още много други.
https://www.youtube.com/watch?v=LSSHuMHbCWo (топ коментара на видеото също е доста полезен)
Wikipedia страниците на DMI, southbridge, northbridge и PCI express.  Не съм изчел всички страници
подробно, ама схванах главната идея.

vsyscall vs vdso: https://stackoverflow.com/questions/19938324/what-are-vdso-and-vsyscall Изпозлват
се за map-ване на kernel информация към файлове с цел избегване на kernel call. vsyscall винаги
map-ва към един и същи файл и не е secure.


ioperm and iopl: https://tldp.org/HOWTO/IO-Port-Programming-2.html
https://stackoverflow.com/questions/23888520/what-is-i-o-port-i-o-port-address-is-that-address-a-part-of-ram
Доколкото разбрах всяко I/O устройство има порт, на който можем да го достъпим и да четем и пишем на
него.  В зависимост дали имаме memory mapped или isolate I/O тези портове може да са адреси от
адресното пространство за паметта или да имат отделен сигнал, който показва за това дали са за I/O
или за RAM-та.

Micro code:
https://en.wikipedia.org/wiki/Microcode#:~:text=Microcode%20is%20a%20computer%20hardware,set%20architecture%20of%20the%20computer.&text=It%20also%20facilitates%20the%20building,the%20complexity%20of%20computer%20circuits.
Код на ниво по-ниско от асемблер. Превежда машинни инструкции към операции на ниво схеми.

MTRR:
https://en.wikipedia.org/wiki/Memory_type_range_register#:~:text=Memory%20type%20range%20registers%20(MTRRs,provided%20by%20most%20modern%20CPUs.
регистри контролиращи до кои memory range-ове има достъп процесора. Заменено е в по-нови системи с
PAT = MTRR++.  https://github.com/torvalds/linux/blob/master/Documentation/x86/mtrr.rst

Погледнах writing policies какво са за write back, write-through и write-combined, както и какво е
PAT.  https://www.mjmwired.net/kernel/Documentation/x86/pat.rst
https://en.wikipedia.org/wiki/Cache_(computing) write-back се пише в кеша и ако друг процесор се
опита да достъпи информация я записва в по-ниските нива от паметта.  write-through записва и на
двете места едновременно(по-ниски нива и кеш) write-combined прави много записи наведнъж.

Разбрах, че има флаг, с който програма влиза в supervisor mode: в този режим програмата има повече
привилегии и може да прави повече системни извиквания.
https://en.wikipedia.org/wiki/Protection_ring#SUPERVISOR-MODE

SMAP: https://en.wikipedia.org/wiki/Supervisor_Mode_Access_Prevention - главно се използва от intel
и може да маркираш зона от паметта, да trigger-ва trap, когато бъде достъпена. Така програми в
superuser mode нямат достъп до цялата памет и можеш да се защитиш от атаки.

Най-накрая минах и страницата за feauture-ите обвързани с процесори, а съм само 11 часа навътре в
работа по домашното. Честно казано опциите са бая и започна малко да ми писва като се замисля
колко ми остават. Сега ще си почина малко и ще продължа, но ако искам да ми стигне времето и да не
умра преди да съм стигнал последната опция в менюто ще е добре да спра да чета толкова подробно в
нета. Не знам кърнъла дали ще работи, защото бая опциики спрях и не знам дали ще ми стигне времето,
ама ще се постарая да прегледам останалите и поне малко да ги разбирам.

Проучих transactional memory: https://stackoverflow.com/questions/11255640/what-is-transactional-memory
Алтернатива на механизмите за синхронизация учена по ОС. Маркираме код като една транзакция и докато не
свършии транзакцията не се допуска друга програма да прави промени. Ако се опита, тя се rollback-ва до 
момента преди да опита и пробва пак. Това води до много overhead.

Прочетох какво е ACPI: https://en.wikipedia.org/wiki/Advanced_Configuration_and_Power_Interface
Разбрах, че е power management система, която иска да сложи power management-а в ръцете на
операционната система(OSPM), а не на хардуера. Работи с ACPI байт код(turing complete).  Линус
Торволдс не го харесва. Някакъв пич го е сравнил с троянец. Ама като гледам се е наложил като
systemd.  Като цяло acpi съм го спирал и преди, когато дебъгвах проблеми с лаптопа, така че знам, че
не е страшно.  Вероятно ще го спра след като му разгледам опциите с цел минималност. Разбрах, че ако
го спра няма да мога да правя software shutdown, което е леко смотано, ама пак ще го спра XD.
Прочетох и за ACPICA: https://acpica.org/

CPU Frequency scaling: Спрях го, защото пишеше, че ако съм несигурен мога да избер N със сигурност,
а и не се нуждая от променлива честота на прецосора или да променям честотата от user space-а. Това
отдолу е една опция, за които трябваше да прочета повече от написаното в menuconfig: P-state:
https://software.intel.com/content/www/us/en/develop/blogs/what-exactly-is-a-p-state-pt-1.html

CPU Idle: https://www.kernel.org/doc/html/latest/admin-guide/pm/cpuidle.html Това изглежда много
интересно, но не го прочетох тялото. Малко съм поуморен от толкова теория. Иначе разбрах, че има
idle процес, който процесорите execute-ват, когато нямат задачи. Той пита governor дали има idle
state, в който може да влезне процесора. Gevernor-а избира оптималния и го праща на driver система,
която изпълнява получените инструкции. Това се нарича idle loop.

BUS опциите не ги разбрах съвсем и не знам спецификите на системата ми. Ще оставя тези по
подразбиране.  Доколкото видях 2 от тях бяха за legacy. Едната беше препоръчителна(ISA-style DMA) и
пусната по подразбиране.  Последната беше за framebuffer-и, които доколкото се разбрах се използват
при стартиране на ОС-а за графическо представяне на информация за debugging. Пишеше, че не е
задължително, но е препоръчително. 

За binary emulation: оставих поддръжката на стари 32 битови програми. x32 обаче не го бях чувал и
изглежда интересно.  Доколкото разбирам позволява на програмите да използват 32 бита памет, но с
повечето регистри на x64 и така намляват използваната памет.  Тук вторият отговор беше доста добре
описан според мен:
https://stackoverflow.com/questions/7635013/difference-between-x86-x32-and-x64-architectures#:~:text=x32%20is%20an%20ABI%20for,larger%20register%20set%20of%20x86_64.
Реших, че няма смисъл да го пускам. Тук не гоня памет.

Реших, че виртуализация не ми трябва и я спрях. И без това не харесвам virtualbox.

Разбрах какво е gcov:
https://en.wikipedia.org/wiki/Gcov#:~:text=Gcov%20is%20a%20source%20code,Compiler%20Collection%20(GCC)%20suite.

За arhcitecture dependant options не съм пипал много. Спрях някои защити от атаки. Kprobes спрях
също. Другото го оставих default-но Прегледах и част от man page-а за mmap(2), защото имаше опции за
offset-и за ASLR. ASLR го спирах, където можех. Виждах, че като цяло е повече рандомизиране на
паметта с цел предотвратяване на атаки.

Модулите може да се премахнат с цел да оставя само най-нужното, ама реших да ги оставя. Все нещо
мога да реша да оставя като модул, макар че вероятно, ако е модул не е жизненоважно. След време мога
да си променя мнението. Оставих само опцията да мога да спра модули след като ги заредя. Все пак е
малко смотано да нямам това, като избор, ако искам модули.

Защото искам операционната ми система да работи оставих block layer. Харесвам си блоковите
устройства :D.

Zoned storage:
https://blog.westerndigital.com/what-is-zoned-storage-initiative/
Нов вид на подреждане на записите върху диска оптимизирано повече за памети като NVMe доколкото
разбрах. Досега те са били backwards compatible. Идеята е, че паметта се дели на зони и те са
последователни. Полезно и за някакъв нов вид записване върхъ твърди дискове(SMR), където
информацията е записана върху track-овете така че да се преплита(overlap-ва). Доколкото разбрах това
позволява повече данни да се запишат. Тогава обаче може да се изтрие само цялата преплитаща се инфромация
и това може да е зоната.

Виж това утре: https://lwn.net/Articles/405076/
https://wiki.archlinux.org/index.php/Self-encrypting_drives
https://en.wikipedia.org/wiki/Opal_Storage_Specification

Уморен съм и стигнах днеска до близо половината на главното меню, като се надявам, че не остава
свръх много. Със сигурност в другите опции няма да задълбая толкова, че искам да си компилирам
kernel-а и да видя, че работи преди да предам. 

На следващия и последен ден(неделята) съм и видях линковете от горе за opal и SED и writeback throttle.
Реших да погледна още колко опции ми остават и да разгледам менютата и малко ми прилоша. Твърде много опции
за мрежи и networking има. Цялостно са мега много. Няма как до края на деня да ги изчета и да направя нещо
работещо. Мисля просто да се откажа от външен research и ще разчитам на това, ако го разбера от description-а
в menuconfig супер иначе няма да пипам опцията. Мен ми се струва супер много. Особено като много от концепциите
не съм ги чувал и има голяма вероятност и да ги забравя след време понеже главно чета теорията зад тях. Линковете 
от сега нататък ще са ресурси, които мога да прочета за в бъдеще, ако имам време.

Не спрях поддръжката на мрежи, защото пишеше, че доста приложения разчитат на нея дори да не планирам да се
връзвам към други машини. Тук има безумно количество опции и честно не знам дали мога да ги прочета всичките.
Само за Quality of Service видях 30 модула, които мога да включа.

Netfilter спрях някои опции, които не бяха задължителни. Също реших, че iptables не ми трябва, защото няма да правя
сериозно филтриране на пакети с chain-ове и тн. Все пак firewall-а е полезно нещо и не го махнах като цяло, макар че
веряотно мога XD.

QoS го спрях. Разгледах опциите за приоритизация на пакети(повечето не съм ги чувал, само class based queueing ми говореше нещо),
но цялостно мисля, че ако целим минималност FIFO е файн, а и не очаквам много трафик, така че да е проблем.

High availability също не ми трябва.

Изчетох секцията с мрежите. Имаше неща, които знаех, като Wifi, WiMax, bluetooth, iptables и много
други, но имаше и доста неща, които ми се сториха или свръх специфични или не ги бях чувал досега.
Примерно B.A.T.M.A.N протоколът XD. Реално вече ми е леко писнало да чета опции и хем ги чета хем не
знам дали всичко разбирам, а и без повече четене не мисля, че има как да стане така или иначе. За
това обаче нямам времето.

Починах си малко и започвам с по-кратките секции като файловата система.
Оставих само ext4 като файлова система, защото само нея имам в момента на машината. Оставих и
псевдофайловите системи, като procfs sysfs tmpfs, защото те вероятно се използват от други програми
за събиране на информация за кърнъла. Изглеждат ми важни за една линукска машина. Не планирам да
използвам NFS и затова го спрях. За charsets пуснах кирилица на няколко места. Не знам дали е нужно,
защото пишеше, че са за FAT системо, но може да е полезно за флашки и прочие. Технически не е
минималност, ама вече не мисля, че целя свръх миниманлност както в началото, а по-скоро да тръгне
XD. Причините са липса на време и не съм мазохист.

За криптография не знам свръх много. Опции за https не съм ги спирал. x.509 сертификатите са си
важни. Хеширащи алгоритми и прочие също. Ако не искаме достъп до интернет или да си криптираме
данните, вероятно не са от значение и могат да се спрат. Като цяло за криптография не знам много.
Наясно съм с diffie hellman как работи. Знам, че RSA е известно за public key cryptography. Как
работи или съм забравил или не съм знаел. AES знам, че е предпочитания криптиращ алгоритъм и че DES
и 3DES са стари, ама точно защо вече не мога да кажа. HMAC знам, че се използва за верифициране на
пакети. Курса по криптография следващия семестър може да го запиша малко да се образовам как работят
тези неща. Разгледах все пак част от опциите в тази секция, но са много, а и повечето или имат
описание от типа: пусни AES, или ако е по-подробно изисква малко повече разбиране или включва просто
кой е създал алгоритъма. В крайна сметка нищо не промених или спрях.

За library routines повечето опции бяхза CRC, което знам, че се използва при мрежови фреймове, да се
потвърди, че fram-а не е бил модифициран по време на изпращане. Тези опции depend-ваха на нещо друго
и бяха пуснати от друга функционалност, която не съм спря вероятно в секцията за криптография.
Другото беше за xz компресиране, което също беше пуснато от друг feature, които зависи от него.
Оставих за него само поддръжката за x86 какъвто е моя процесор.

Kernel hacking: цялата секиця са 300 дебъг опции или допълнителни файлове, които може да използваш,
да се сдобиеш с повече информация за kernel-а. Прегледах близо половината опции и повечето знам, че
няма да ги запомня така или иначе и реших да спра и без това ми остават още 2 огромни менюта. От
това, коеот прочетох разбрах и че мога да пусна неща като debugfs файлова система от там, да пусна
runtime, unit тестове за kernel-а. memory, cpu тестове и общо взето каквото се сетиш. Има и всякакви
trace-ове, така че да можеш да видиш какво е довело до счупването.

Останаха две подменюта. Едното средно дълго(Security options) другото безумно дълго(Device Drivers).
Ще започна с по-краткото, че ако отворя Device Drivers отново сигурно ще ми прилошее колко остава.

Security options накратко опции за SELinux, Security hooks и други. Реших Security hooks да ги спра,
което спря и SELinux. За миналото домашно имаше SELinux опции, които бях променял за boot-ването по
мрежа и прочие, но осъзнах, че не знам какво точно е самото SELinux и прочетох в уикипедия, че служи
за Access Control политики. Обвързани са му идеите с NSA(National security agency) в щатите.
Доколкото разбрах SELinux няма концепцията за root потребител. Идеята е, че по принцип линукс
разчита на това всичките му компоненти да работят, да няма security проблеми. Докато SELinux го
прави така, че само системата за security политики трябва да работи, да няма проблеми.
Всички опции за integrity ги спрях. В реална среда вероятно са важни, но с цел минималност няма да
ги пускам. Прочетох и какво е LSM: https://en.wikipedia.org/wiki/Linux_Security_Modules Явно просто
security module.

Остана най-дългата секция: Device Drivers
Прочетох какво е FPGA и си спомних, че го бяхме учили по КАРХ:
https://en.wikipedia.org/wiki/Field-programmable_gate_array#:~:text=A%20field%2Dprogrammable%20gate%20array,term%20%22field%2Dprogrammable%22.
Прочетох какво е PHY device:
https://en.wikipedia.org/wiki/PHY#:~:text=A%20PHY%2C%20an%20abbreviation%20for,in%20a%20network%20interface%20controller.
Имаше драйвери специолно за thinkpad-ове(моя лаптоп е такъв) за hdaps. Стана ми интересно и прочетох
това: https://wiki.archlinux.org/index.php/Hard_Drive_Active_Protection_System
Видях и секция с драйвери, които се разработват и я разгледах, за да не разбера за какво са XD.

Имаше доста опции за драйвери за часовници(RTC). Някакакси не се замисляш колко неща има в компютъра
първоначално. Толкова много чипове има само за синхронизация на времето и часа. Друг пример са
300-та LED драйвери и трегери, с които можеш да конфигурираш лампичките на лаптопа XD.

Всичките конкретни опции за хардуера се отказах да ги чета. Вероятно са доста и повечето не ми
говорят нищо. Като цяло не знам всичко, което е на машината ми какво е. Всеки чип или всеки тип
BUS/PCI каквото и да било не ги знам. Цялостно вероятно мога да го видя с lshw и да пусна всички
релевантни опции, ама това ще е много занимавка. Към момента в тази секция основно спирам тези
опции, които пише, че е безопасно да спра. Официално разгледах и device drivers секцията. Не
прочетох всичко в нея, защото малко ми е писнало вече, а и повечето опции бяха за специфичен
хардуер, но прегледах поне какво има.

Време да build-на и да не работи :D. Тревожи ме, че memory manager-а ми е SLOB. Не знам дали това
няма да доведе до бая проблеми или много бавна система. Без риск не е забавно така или иначе така че
ще пробвам. Ако е проблем ще избера SLUB.

Build-нах успешно кърнъла, но като го пуснах се оплака, че иска cgroups и autofs. Явно съм избързал
като съм ги спрял.  Не съм сигурен къде точно се използват. Може би защото имам докер едното, за
другото не знам. Бях инсталирал nfs клиент за миналото домашно. Може би е нещо заради това. Честно
не знам. Може и да е от дистрибуцията.

Като ги пуснах пак получих грешка. Гугълнах я и намерих, че може просто да съм спрял някои
важни опции.  Този линк ме спаси: https://lwn.net/Articles/672587/ . Едната грешка беше, че съм
спрял singalfd, а другото, че бях спрял timerfd. Ще е добре да пише кои неща са жизненоважни, да
тръгне. Вярно е написано, че не е добре да ръчкам тези опции, ама не е ясно къде се използват според
мен. Особено, ако не си запознат по-надълбоко.  Иначе сега пиша като работя на нови си kernel :D.
Има грешки при тръгването на операционната ми система, но работи.

Цялостно това не ми беше любимото домашно. Четенето в даден момент ми дойде много, а и вероятн аз
съм си виновен, че четох твърде подробно в началото. В момента имам 27 часа вкарани за тази задачка.
Повече дори от миналото домашно и малко изпуших накрая и на няколко момента както се вижда горе. Все
пак беше интересно според мен и си заслужаваше, ама нещо по-конкретно от минимален kernel може да е
по-интересна задачка. Не че разбирам достатъчно, за да измисля какво XD

Неща, които нямам времето да прочета, но може да са интерсни или полезни за в бъдеще:
https://en.wikipedia.org/wiki/TCP_congestion_control
https://www.kernel.org/doc/html/latest/netlabel/index.html
https://en.wikipedia.org/wiki/IPsec
Wikipedia SNMP(май беше за мониториране на мрежата, но не помня специфики)
https://wiki.nftables.org/wiki-nftables/index.php/Main_Page
ELF: https://tldp.org/HOWTO/Program-Library-HOWTO/introduction.html
https://en.wikipedia.org/wiki/6LoWPAN
Service function chaining
L3 master device: https://lwn.net/Articles/658471/
real-time OS: https://en.wikipedia.org/wiki/Real-time_operating_system
Infiniband: https://en.wikipedia.org/wiki/InfiniBand#:~:text=InfiniBand%20(IB)%20is%20a%20computer,both%20among%20and%20within%20computers.
