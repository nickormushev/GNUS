След прочитане на заданието съм доста въодушевен. Нямам търпение да започна. От доста време на сам
ми се е искало да науча или nginx или apache, а в най-добрия случай и двете. Тук виждам, че трябва
да си генерирам и сам сертификати. Спомням си преди време, че един колега работеше по нещо подобно и
имаше нещо наречено certbot/let's encrypt, което той използваше при генерирането на сертификатите си.

Като първа стъпка реших да инсталирам nginx и да мина към четене на документацията му. Спомням си, че
преди време ми бяха казали, че документацията на nginx не е много добра. Мога и да се бъркам, но сега
е дошло времето да разбера от личен опит. Гугълнах nginx getting started и ми излезе този линк:
https://www.nginx.com/resources/wiki/start/ . Изглежда ми като добра отправна точка. Започнах с
четенето на линковете в началото на страницата, които ми напомняха малко повече на блогове с
упътвания отколкото на цялостна документацията. Хареса ми този цитат:
The first step in making Nginx work for you is to not follow 95% of the guides found on Google. ,
който взех от тук: https://michael.lustfield.net/nginx/dummies-guide-to-setting-up-nginx
Явно е имало нещо вярно в това, което съм прочел за документацията или явно по-скоро има доста
грешна информация. Линка с готиния цитат ми се стори доста безполезен. Цялостно не разбирам
достатъчно базовите концепции, за да го разбера добре и ми се струва просто като три пльокнати
конфигурации за php, mysql и nginx, което поне докато не навлезеш в една технология не ми се струва
от полаза, а и идеята ми е да разбера как nginx работи, а не само да го пусна.

Втория линк, който ще разгледам е това: https://blog.martinfjordvald.com/nginx-primer/
Той на пръв поглед ми изглежда по-обещаващ. Този цитат ми изглежда интересен:
The most important is that nginx is a reverse proxy first and HTTP server second, its
first concern is not files but rather URLs, this changes the way we have to configure nginx.
Още не го разбирам съвсем, но ми изгежда ключов за разбирането на nginx как работи. Като цяло не
трябва да си мисля, че е http сървър, а по-скоро reverse proxy, което може и да е http сървър. Ще си
имам едно наум за това отсега нататък.

Разбрах, че конфигурацията на nginx се дели на блокове. Директивите в по-горните блокове се
наследяват от по-долните като стойности по подразбиране, които по принцип могат да се предефинират.
Затова се препоръчва да пишем директивите си на най-високото ниво, защото има по-малко объркване и
повтаряемост на код. Основните блокове са: http -> server -> location. Има и event и root блок,
които са по-специализиране, като в root блока се сдържат http и event блоковете.

Първите две директиви, коит са разгледани са server_name, което се използва за match-ване на domain
и root, което казва, в коя папка да се търсят файлове. Също така разгледахме как да направим server,
който мачва по подразбиране. Съответно, ако нито един домейн не мачне някой сървър може на listen да
подадем default_server и тогава ще се мачва той. Също така server_name _;  беше обяснено, че общо
взето значи, че този сървър няма да match-ва нищо. _ общо взето означва нищо. Така никога няма да
бъде използван този сървър и това явно в доста упътвания се срещало. Също така за server_name се
match-ва по longest exact match, т.е. може да имам няколко сървъра за един домайн, но ако даден
string съвпада точно с някой server_name и някой regex, то това, с което съвпада ще бъде
приоритизирано.

locations: Позволяват match-ване по самите пътища(path), на uri-то. Така можем да redirect-ваме
например заявки от един път към друг. Също така имаше препратка на различния синтаксис за
match-ване. ~* - case insensitive regex, ~ - case sensitive regex, ^~ - ако match-нем това то
regex-овете не се преглеждат. = - exact match. Търсенето на location-и според документацията:
https://nginx.org/en/docs/http/ngx_http_core_module.html#location върви така:
    1) Проверяваме за най-дълъг match-ващ префикс и си го записваме
    2) Ако попаднем на exact match с =, горното търсене веднага терминира
    3) Ако най-дългия match на търсенето в 1) е с ^~, regex-ите не се преглеждат
    4) Преглеждат се regex-ите стига да не сме в 2) или 3) и влизаме в първото съвпадение
    5) Ако няма съвпадащ regex, то се избира най-дългия префикс намерен в 1)

По принцип location-ите могат да се влагат един в друг предполагам за по-точни match-ове. Изключение
прави location-и с префикс @, който дефинира блокове, които се използват зе redirection само. Ако
location се дефинира и завършва пътя(path-а) му с / , заявки, които не съдържат това / по
подразбиране с пренасочват към пътя с /. Ако това е нежелано, може да се заобиколи, ное е готино, че
го има.

След прочитането на документацията дочетох блога. Обясняваше как се използва nginx за зареждане на
php файлове и също така се използваше fastcgi за тези цели. Спомням си от заданието за компилиране
на php приложение, че бях прочел малко за това какво е cgi, но не бях задълбал. Цялостно знам, че е
процеса, който е връзката между nginx и application server-а. Понеже има доста типове application
сървъри може да има различен cgi специфичен за него. Примерно един за php или един за node и други.

Разгледахме и try_files директивата, която проверява дали съществуват файлове или директории, като
първия match се използва за обработване на заявката. Също така може да се даде отговор по
подразбиране, ако никой файл не съществува. Това може да е error code или uri.

Цялостно втория линк беше една идея по-полезен. Припомни различните видове равенстов и как работи
match-ването на nginx. Това го учихме и с Хакман, ама тотално го бях забравил кое за какво беше.
Това смятам, че се дължи главно на липса на практика. 

Накрая разгледах този линк: https://nginx.org/en/docs/#introduction , който ми изглежда малко повече
като официална документация и вероятно ще е по-полезен. Следвайки линковете в него открих как да
инсталирам официалните stable nginx пакети за debian: https://nginx.org/en/linux_packages.html#Debian
Не знам обаче дали не е по-добре да използвам repository-тата, който имам на машината. Според мен
горното е супер, като най-нова версия, но може да не е съобразено със debian stretch. По принцип
мисля, че е по-добре да използваме пакетите на package manager-а като цяло или ако реша да си
направя backport, но това може да се окаже сложно в зависимост от брой dependency-та и подобни. С
цел да не хаба време просто ще направя apt install nginx. Иначе винаги, ако съм мазохист мога и от
source да си build-на пакета XD: https://nginx.org/en/docs/configure.html . Не че самото build-ване
ми е проблем след едно от домашните, ама винаги има шанс нещо да се счупи, а и пак го има проблема с
dependency-тата.

Този линк определено е по-полезен: https://nginx.org/en/docs/beginners_guide.html
Тук е описано, че nginx се състои от един мастър процес и няколко worker-а, които обработват заявки,
докато мастъра се грижи за конфигурацията. Описано е как се пуска и как се спира nginx сървъра с
различни сигнали. При презареждане на конфигурацията сървъра убива child процесите един по един и ги
заменя с нови. Самите процеси умират gracefully, като обработват последните заявки, кото са получили
преди да умрат и през това време спират да приемат нови.

Описано е, че конфигурацията на nginx се състои от два основни вида директиви и това са блокове и
прости(simple) директиви. Простите директиви се състоят от име и параметри, разделени с интервали, а
сложните са като функции с къдрави скоби все едно. Ако в тях, могат да се вложат други директиви те
се наричат контексти, като някои сме ги виждали досега, а именно location, server и http. Ако дадена
директива не се съдържа в друга, тя е част от main контекста.

В линка са описани две примерни конфигурации за nginx, като reverse proxy и връщане на статични
файлове, което ние реално искаме. Също така е показана и примерна fastcgi конфигурация. Понеже ми
беше интересно дали $query_string и подобни са някакъв тип променливи гугълнах и видях, че съм прав,
като намерих този списък с променливите на nginx: https://www.javatpoint.com/nginx-variables, което
може да е от полза по-късно. По-голямата част от прочетеното за примерните конфигурации го има и
по-горе. Интересно е, че по подразбиране сървъра слуша на порт 80 и не се налага експлицитно да му
го казвам. Иначе май е само това. Прочетох и малко за fastcgi params да си изясня синтаксиса му тук:
https://nginx.org/en/docs/http/ngx_http_fastcgi_module.html#fastcgi_param . Видях, че с главни букви
е името на параметъра а после е стойността му получена от променливите на nginx.

Разглеждайки другите линкове на централната beginners_guide страница видях какво точно са server
директивите: https://docs.nginx.com/nginx/admin-guide/basic-functionality/managing-configuration-files/
Ние реално това го разглеждахме на лекции, че се пускат няколко виртуални сървъра, които приемат
заявки към различни endpoint-и, които при apache се наричаха virtual hosts, а тук са server-и.

Тук https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/#overview са описани
различните видове load balancing между сървъри, които отговарят на даден адрес. Съвърите могат да се
групират с upstream директивата. После с proxy_pass/fastcgi_pass или нещо подобно може да се
дефинира се пренасочат заявките, които са match-нали даден server location натам. Тук е описано как
може да се използва балансиране на база брой заявки или просто round robin да се използва. Има и
други опции, като някои бяха май само за nginx plus, което май е платената версия на nginx. Описани
са и zone директивите, които позволяват на worker сървърите да споделят статистики за заявки. Това 
работи като се заделя някакво количество помет, която се използва от всички worker-и. Така примерно,
ако сме конфигурацирали max_fail или нещо подобно няма да имам броя на сървърите * max_fail, като
най-лош случай, за да режем заявката, а ще е само max_fail, както се очаква. Тази секция мисля, че
главно е полезна заради upstream. Миналата конфигурация на nginx мисля, че го използваше по
подразбиране с proxy_pass до upstream нищо, че тогава използвахме само един сървър. Вероятно идеята
е била, че може с времето по-лесно да го разширим, а и ми изглежда по-изчистено, макар че мога и да
греша.

Цялостно в документацията са описани всякаки неща, като health check-ове и други типове load
balancing, като tcp и udp. Засега мисля да не задълбавам в тях, защото не са релевантни за задачата.

В процеса на четене още в първата или втората статия бях видял нещо за sites-available и
sites-enabled. Реших да се опитам да видя каква беше разликата между двете, защото по спомен в тази
директория се слагаха конфигурациите на сървърите, ама не съм на 100% каква беше семантичната
разлика. Спомням си, че можех със symlink-ове да сложа файлове и в двете директории, които да са
синхронизирани. Тези конкретни знания са от един приятел, с който правихме проект и тогава той
наконфигурира nginx сървър. На мен ми се наложи да разгледам конфигурацията, ама тогава не задълбах
и затова сега ми е кеф, че най-накрая ще го науча. За тази цел гугълнах и видях това:
https://stackoverflow.com/questions/21812360/what-is-the-difference-between-sites-enabled-and-sites-available-directory
sites-enabled са работещите server-и, до които клиентите имат достъп. sites-available са готови, но
не са достъпни все още до външния свят. Един от хората отговорили на въпроса също спомена, че е
добра идея да променяме само в sites-available, а не в enabled, защото vim и нещо друго подобно
могат да създадат свои файлове, които да прецакат конфигурацията.

Разглеждайки getting started намерих това: https://nginx.org/en/docs/http/configuring_https_servers.html,
което вероятно ще ми е доста полезно за домашното. Все пак https ще конфигурираме тука.

Това ще го видя утре, че в момента съм уморен, но изглежда интересно:
https://nginx.org/en/docs/stream/stream_processing.html . Този линк описва какво се случва, като
nginx получава tcp/udp request. Кои модули и в каквъ ред се извикват и така нататък. Опитах се сега
да го прочета, ама мозъкът ми не компилираше.

Понеже си спомнях, че май имаше някакъв препоръчителен начин да си разпределя файловете гугълнах и
намерих това: 
https://subscription.packtpub.com/book/networking_and_servers/9781849514965/1/ch01lvl1sec12/splitting-configuration-files-for-better-management
От него разбрах общо взето каквото знаех досега. Слагай сървърите в sites-enabled и available и също
така научих че може да си сложа fastcgi конфигурацията в отделен файл. Цялостно след инсталирането
на nginx самата директория с конфигурацията си е по подразбиране разделена така плюс минус някой
друг файл. Има и modules-enable и available, които предполагам работят на същия принцип, като sites,
а има и отделен конфигурационен файл за fastgi. Реално остана само да наконфигурирам сървъра вече
май. Повечето идея мисля, че ги разбриам. Разбрах какво са server/location/http/upsteam/main
блоковете. Добра идея ще е да видя и event блока какво е, може би, и линковете от по-горе и 
после май съм ок да започвам да конфигурирам. А трябва и да прочета за letsencrypt и certbot преди
това, но все пак мисля, че за начало ще се опитам да подкарам nginx на порт 80 да слуша за заявки.

Понеже исках да имам яка html страница гугълнах и откраднах на един човек кода: 
https://codesandbox.io/s/star-wars-crawl-b2m8u?file=/index.html:0-5439
Така си имам яка star wars title screen анимация на сайта XD. Съмнява ме, че толкова хора ще ми
гледат сайта, че да е проблем.

Още един линк с обяснение как да конфигурирам static content, като отговор от nginx:
https://docs.nginx.com/nginx/admin-guide/web-server/serving-static-content/
Понеже се изкуших реших да конфигурирам november.fmi.fail да работи. Трябва още да прочета преди да
съм доволен, но накратко стана. Успешно добавих index.html във /var/www/november директорията.
Създадох сървър под sites-available и му направих symlink под sites-enabled. С директивата
server_name match-нах по november.fmi.fail и сложих root /var/www/november. Накрая в location / 
сложих index index.html, за да връщам желания файл и воала работи след презареждане на
конфигурацията, защото се оказа, че сървъра е зареден по подразбиране. Цялостно е доста готино. Не е
сложна конфигурацията засега, ама вярвам, че ще стане по интересна като вкарам сертификатите.

Утре или другиден ще прочета последните горни линкове малко по-подрбно все пак стига да имам време, 
че засега само ги прегледах отгоре отгоре, за да намеря каквото ми трябва, защото съм уморен.
Цялостно все пак мисля, че разбирам какво правя, поне засега :D.

Така вчера си набелязах тези три линка споменати по-горе и сега мисля да ги разгледам:
1)https://nginx.org/en/docs/stream/stream_processing.html 
2)https://docs.nginx.com/nginx/admin-guide/web-server/serving-static-content/
3)https://nginx.org/en/docs/http/configuring_https_servers.html
Ще започна в реда, в които съм ги подредил.

1)Като започнах да го чета видях, че се реферира нещо наречено proxy protocol. Съответно аз си нямах
и на идея какво е това и започнах да дълбая в линковете и стигнах до това:
http://www.haproxy.org/download/1.8/doc/proxy-protocol.txt В него пишеш, че proxy protocol-а е
създаден в резултат на така наречените dumb proxies, които говорят само протоколи от ниско ниво и не
се интересуват от това какво прехвърлят. Това обаче е проблем за сървъри, като nginx или haproxy,
защото на тях може да им е важна информацията в header-и, като x-forwared-for, който съдържа ip
адресите на proxy-тата, които са прехвърлили заявката и адресът на клиента, от когото е дошла
заявката. Обаче dumb proxy-тата не разбират от http и те няма как да сложат този header и затова е
създаден протокол от по-ниско ниво, наречен proxy protocol, който се използват за записване на тази
информация. Тя се слага от клиента преди пращане и от сървъра при отговор и така двете машини имат
цялата информация, която им трябва. Тази функционалност може да се пусне на даден порт с proxy_port
параметъра за listen директивата.

Сега обратно към линка. В него е описано стъпките на установяване на tcp/udp сесия с nginx сървъра.
Тази връзка се обработва на няколко фази. Първо се използва proxy protocol-а за подмяна на
клиентския адрес и порт (Post-accept). После се проверява брой връзки от даден адрес дали не
надминава максималния позволен брой, който е зададен с limit_conn директивата. Също се задават
стойности на променливи зададени със set директивата (Pre-access). Проверява се дали клиента има
право на достъп или по-точно дали има някоя диреткива от типа deny или allow, която му позволява да
достъпи (Access). След това се изгражда SSL сесия между клиента и сървъра в SSL фазата. Поправка
това е грешно SSL трафика се дрекриптира в тази фаза, а не се прави сесия. Прочетох какво значи
терминиране на SSL тук:
https://avinetworks.com/glossary/ssl-termination/#:~:text=SSL%20termination%20is%20a%20process,handle%20many%20connections%20or%20sessions.
Анализира се с помощта на preread модула информацията в preread буфера, която може да казва точно
кой virtual host/server искаме да достъпин на nginx, като се използва Server Name Indication(SNI)
протокола, който е extension на SSL. Също така може да съдържа и информация за това дали искаме да
използваме TCP или UDP дадена от Application-Layer Protocol Negotiation Extension-а(ALPNE) за TLS.
Информацията за extension-ите взех от тук: 
https://tools.ietf.org/html/rfc7301 и https://tools.ietf.org/html/rfc6066#section-3
Информацията за SNI изглежда интересна, защото май е обвързана с един от въпросите, които май
зададоха по време на лекцията как се изгражда SSL сесия с различни virtual host-ове. Ако разбирам
правилно сесията е със сървъра, а SNI се използва, за да се определи с кой virtual host да говориш.
Тук мога и да греша. Все още не съм изчел секцията за SSL и Letsencrypt не съм гледал, а и мога
грешно да помня въпроса. Така или иначе нека продължим. Следващата фаза се нарича Content и в нея се
взима решението на къде да се препрати заявката. После следва последната фаза, която се състои в
това nginx да запише лог за установената връзка с клиента. Това вече са всички етапи през, които
nginx минава при приемане на заявка и препращането й. Цялостно доста интересно. Вероятно в самите
етапи може да се задълбае, но е добра обща представа на нещата.

2) Цялостно ми се струва като добра идея да разгледам секцията за конфигуриране на nginx за web
server, която е част от admin guide-а. Снощи явни, като съм бил уморен дори не съм загрял, че има
такава секция. Виждах само тези за мониториране и load балансиране. 

В процеса на четене леко се мотивирах да оправя условието за това, че файловете започващи с .ht
трябва да са забранени и започнах да си играя с regex-и. Цялостно беше голям обърквация. Като
резултат получавах само 404 и не ми беше ясно защо. Първоначално исках на този друг път да му върна
просто различен index, но това не работеше и цялостно още не знам защо. Мислех, че самия regex е
проблема и 300 години търсих и си мислех, че нещо не се презарежда конфигурацията, но тя се е
презареждала. Просто не се е връщал index файлът, което мен ми бърка в душата. Цялостно се опитвах и
с абсолютни пътища да мачвам и със регекси и пак получавах само 404 и въпросът е защо? В крайна
сметка на късмет match-нах по regex и видях, че с deny all се връща различно съобщение от това,
което получавах с index. Реших да променя малко страницата за 403 също така, за да изглежда готина и
намерих error_page директивата в документацията:
http://nginx.org/en/docs/http/ngx_http_core_module.html#error_page и това си бачка и изглежда
готинко. Сега се замислих, че вероятно не разбирам съвсем какво прави index директивата и реших да
прочета и нейната докуменатция тук: http://nginx.org/en/docs/http/ngx_http_index_module.html#index .
След прочита ми още не съм на 100% какво става. Разбрах, само че индексът влияе и на другите
location-и. Там се показва, че location / ще направи internal redirect към /index.html, защото това
е индексът. Това не ми е ясно до каква степен работи и за какво важи. Трябва наистина да продължа с
четенето. Все пак в крайна сметка regex-а работи и match-вам всички пътища, започващи с .ht, но не
съм доволен, защото не знам още защо.

Продължавайки с четенето прочетох за rewrite директивата, която е сравнително проста. Позволява да
match-ваме по regex заявка и да й променим пътя. После пак се търсия на база новия път нов location.
Тя има два флага last и break, като първия работи както по-горе описах. Ако match-нем пренаписваме
пътя и започваме да търсим нов location, а втория ако го match-нем спираме и не търсим нов location,
ако го match-нем.

Прочетох и за subfilter, което позволява да заместим един string в http response-а с друг. Може да
заместим един път с друг или localhost с валидно uri. Обяснено е, че може и повече от веднъж да
търсим за всеки string, който искаме да заместим със subfilter_once. Това предполагам е като /g на
някой regex.

Разбрах, че и с error_page мога да пренасочвам потребителя към home странцата и сега, ако напише
несъществуващ път ще се redirect-не към /. Това не променя path-а, който са ми подали обаче. Опитах
се със sub_filter да го оправя, като подменя $uri, което се надявах, че е само пътя, ама нещо не ми
се получи. С цел да не губя много време ще продължа напред.

Ахаа като прочетох страницата за static-content разбрах, че index прави това, което си мисля, че
прави. Не знам защо в моя случай не работеше. Иначе научих и че redirect-а, който споменах по-горе
се случва, защото като match-на например / и този location има index index.html, то ще направим
повторен search на /index.html, което пък може да match-не някой друг location, който да го препрати
нанякъде другаде.

Показано е, че мога и с try_files да направя redirect или по-точно да върна default-ен файл, ако
някой не съществува. Така мога да избегна error 404 предполагам, но ми се струва по-добре да има
грешка в response-а, което и в моя случай не се случва, като се замисля(тествах го в бразъра и
получих 200), така че май е все там дали така или с error_page 404 = <uri> или по друг начин. Бих
очавкал пак да получава 404 в моя вариант, ама явно е по-скоро redirect. За .ht обаче си връща 403
:D. Видхя, че try_files поддържа loaction-и дефинирани с @<нещо си> и може да те препраща към тях.

Реших да използвам sendfile, защото ми изглеждаше логично директно да изпращам данните от файла, а
да не ги зареждам в паметта, за да ги изпратя после така или иначе. Ограничих размера на извикването
на sendfile до 1 мб наведнъж със sendfile_max_chunk. Като цяло ми изглежда да работи ок. Изглежда ми
по-оптимално поне на пръв поглед директно в мрежовия буфер да се слага съдържанието на файла, а да
не се зарежда в паметта преди това. Така е по-бързо и май не губим нещо съществено.

Обяснено е, че tcp_nodelay е включено по подразбиране. Тази директива може да се използва за
keepalive пакети само с keepalive_timeout. Идеята му е, че спира delay-а на алогоритъма на Nagle,
който преди време е чакал 20ms преди да прати няколко консолидирани паката, което е по-оптимално за
бавни мрежи. В днешни дни това не е нужно, защото мрежите са доста по-бързи и този delay само
забавя.

Дадени са примери как с netsat -lan да видим колко заявки има в queue-то за nginx и чакат обработка
и показва как да увеличим със sysctl максималния брой на връзките на порт 80. Стойността, която
изберем със sysctl -w net.core.somaxconn=4096 трябва да бъде казана на nginx с backlog параметъра на
listen. В случая backlog=4096.

С proxy_set_header може да променя стойността на някой header на http request по доста лесен начин.
Потвърдих си, че uri-то е само path-а, а не целия url. Поне така беше обяснено на няколко места.
Мислех, че са взаимозаменяаеми по прицнип, ама явно не са.

Обяснено е, че по подразбиране, ако nginx работи като proxy то той буферира целия отговор за даден
клиент преди да му го прати, като отговор на неговата заявка. Това помага с по-бавни мрежи, но ако
искам голяма бързина може да го спрем. Също така може самия буфер да го прави по-голяма или по-малък
с proxy_buffer_size. Директивата за пускане и спиране е proxy_buffering между другото.

proxy_bind - може да се използва за определянето на различен ip address, който сървъра ни да
използва за говорене със upstream/сървъра, на когото сме прокси.

Обяснено е и компресиране и декомпресиране с gzip. Мога отговорите на заявките към мен да ги
компресирам с gzip. Има команда gunzip, която позволява декомпресиране на компресирана заявка. Това
се използва, ако например само 1 от 200 клиент не поддържа компресиране. Също така мога да кажа
колко е минималната големина на отговора при компресия. По подразбиране се компресира само html, но
мога да избера и други типове с gzip_types. Също, ако получа заявка от друго прокси по подразбиране
не я компресира nginx, но ако реша мога да му кажа да го прави с gzip_proxied, което има и
различни флагове, които дават повече грануларност на това точно кога искаме компресия. С gzip_static
казваме на nginx, че ще отговаряме с предварително компресиран файл. Не го компресира в този случай
nginx-а, а ние трябва да го направим.

Последната секция е събирателна и е примерна конфигурация на nginx със django и uwsgi. Тя е полезна за
пример, но не е нещо специално, за да задълбавам. Мисля, че вече съм готов с тази секция и е време
да навлизам в ssl-а. Единственото, което ми хрумва е да видя разликите между различните CGI
алгоритми, като uwsgi, cgi, fastcgi и тн. Знам за какво се използват, но не знам кое кога би било
по-добро. Не помня дали това не го бях чел в php домашното, но имам спомен, че не задълбах тогава с
идеята сега да го направя и ако остане време вероятно ще го направя XD. Видях, че май uwsgi се
препоръчва от nginx хората, като най-бързото от опциите, които предоставят. Все пак бих искал да
попрочета.

3) SSL
От това видео съм научил по-голямата част от нещата, които знам за SSL: 
https://www.youtube.com/watch?v=-f4Gbk-U758&t=1444s По мое е мнение е доста подробно и ясно
обяснено. Цялостно знам, че за handshake-а между клиента и сървъра се използват публичния и private
ключа, които аз ще генерирам. Обаче, за да ми се довери клиента, че няма man in the middle атака аз
трябва да имам подписан ключ от certificate authority. Сега ние ще целим да се сдобием с такъв
подписан сертификат/публичен ключ, с който ще можем да си говрим и да направим diffie hellman key
exchange например и да започнем да говорим със симетричен ключ, което е по-бързо и оптимално. 
В линка за тази секция са описани директивите нужни на nginx, за да му се подаде private ключа за
декриптиране на съобщения и сертификат(crt файл), който да се подаде на клиента, за да го верифицира
той. Също така са описани настройките за кои версии на TLS nginx поддържа по подразбиране. Доколкото
знам трябва да поддържаме TLS v1.2 и v1.3, защото другите вече са deprecated от 2020 година. За
по-сигурно това трябва да го променя. Иначе в линка са описани са няколко проблема, с които може
да се сблъскам и един, с който със сигурност ще се сблъскам:
    3.1) Може да имам сертификат, който е подписан от intermediate CA, а не от well known CA. 
    Тогава трябва да сложа и сертификата на intermediate CA-а, защото може другия браузър да не го
    знае и да ми каже, че сертификата ми е несигурен. Това може да се реши със това да сложа
    сертификата на intermediate CA-а след моя в комбиниран файл със сертификати. Има и алтернативен
    начин с openssl. Показано е как да видим дали openssl bundle е добавен за целия ни certificate
    chain с openssl s_client -connect www.godaddy.com:443 , но това не ми казва как да добавя този
    bundle. Ще се наложи повече да прочета.
    3.2) Главния проблем, който обсъдихме в час. Това е проблема да имаме няколко сертификата за
    nginx за различните виртуални сървъри. Тук са описани няколко решения. Първото е различни ip-та
    за различните сървъри, което аз не мога да си го позволя. Второто е SubjectAltName, което май е
    поле във сертификата, но и това не ми върши работа, защото аз искам да host-вам различни
    domein-и с цялостно различно съдържание. Някакси не ми се връзва да твърдя, че това е
    алтернативно име дори и аз да ги хоствам и двата. Остана само SNI, за което бях прочел по-рано.
    С nginx -V си потвърдих, че моята версия на nginx го поддържа. Остана само да разбера как да го
    имплементирам и дали е нужна някаква допълнителна конфигурация, като засега ми се струва, че май
    не е. В този случай както казах самия браузър ще ми прати, като допълнително поле сървъра, на
    който иска да се върже.
    3.3) Може един и същи сървър да слуша на 443 и на 80, което май ще ми трябва за
    november.fmi.fail.

SNI:
За начина за конфигуриране на SNI първоначално не се усетих, че по-горе, разглеждайки за TCP/UDP
обработката на заявки бях попаднал точно на това, което ми трябва. Затова гугълнах просто и попаднах
на тази примерна гитхъб конфигурация: https://gist.github.com/kekru/c09dbab5e78bf76402966b13fa72b9d2
Съответно на мен тук ми трябва terminating конфигурацията доколкото виждам. Понеже не бях използвал
map-ове досега гугълнах как работят и намерих това: http://nginx.org/en/docs/http/ngx_http_map_module.html
Общо взето си работят като стандартен map. В зависимост от стойността на първата променлива string
се променя стойността на втората, която реферираме в нашата конфигурация. В къдравите скоби сме
сложили самите key value pairs. След още малко гугълване попаднах на това: 
http://nginx.org/en/docs/stream/ngx_stream_ssl_preread_module.html#variables
Това го бях разглеждал както казах и точно от тук бях видял за SNI и че вероятно ще ми трябва, но
не разгледах страницата обстойно и май е време да се поправя. Тук е дадена примерна конфигурация
подабна на онази от гитхъб, който ще ми трябва за моя сървър, така че да отговаря и за
november.fmi.fail и за weber.openfmi.net. Ние реално използваме описания в линка модул, който за
щастие е добавен за моя nginx и с него извличаме от ClientHello заявката от TLS handshake-а
стойността на SNI и после с map-а определяме към кой upstream ще я пратим или кой сертификат ще
използваме както е показано в гитхъб. Това не съвпада обаче с точно това, което ми се искаше да
направя, защото предпочитам да имам два отделни сървъра с отделни конфигурации. За това предстои да
се види как ще стане. Едната ми идея е да съм reverse proxy на себе си и първоначалните заявки да се
поемат от една крайна точка и после да се препращат към другите две или може би да видя дали ще е
много зле да е един сървър и малко повече и по-обстойно да го помисля. Реално с proxy_pass към себе
си мога да използвам и различни ip адреси от range-а 127.0.0.1 вероятно и това би ми свършило
работа. Иначе поне това ми е идеята засега, ама ще видим. Първо искам да видя и какво ще стане, ако
просто сложа различните сертификати на различните сървъри и това дали ще сработи като по магия или
не. Засега обаче май е добра идея да започна с letsencrypt.

Време засега: 11 часа.

На другия ден започнах като инсталирах acme-tiny. Пак използвах stretch версията, за да няма
проблеми с някой backport пакет. После пуснах acme-tiny -h и излезе информация за това как да си
наконфигурирам cron job, който всеки месец да ми генерира нов сертификат. Пишеше, че това е средство
за автоматично генериране на https сертификати с letsencrypt. Същите cronjob-ове и примери ги има в
man страницата, но няма много друга информация. На края на страницата бяха линкнати като ресурси
сайта на letsencrypt и github-а на acme-tiny: https://github.com/diafygi/acme-tiny, https://letsencrypt.org/
като реших да започна като ги разгледам тях.

За да разбера малко повече за acme протокола прегледах protocol overview-то на това rfc: 
https://tools.ietf.org/html/rfc8555#section-8 . В него е описано, че е протокол за изкарване на SSL
сертификати. Дели се на няколко фази. 1) Регистриране с профил към acme сървъра 2) Сървърът
предизвиква клиента, да докаже, че той наистина притежава домейна, за който иска сертификат 3)
Клиентът подава csr на сървъра. 4) Клиентът чака сертификатът да бъде подписан и върнат.

От сайта на letsencrypt тук: https://letsencrypt.org/getting-started/ разбрах, че certbot, за което
бях чувал е техния препоръчан acme client за искане на сертификат. Това обяснява, защо бях чувал за
него, но ние очевидно искаме да използваме acme-tiny.

На сайта на letsencrypt: https://letsencrypt.org/how-it-works/ е описано с малко повече примери как
работи целия процес без да се задълбава много както в rfc-то. Тук са дадени примерни challenge-и за
клиента, като това да се добави dns record или да се добави http endpoint на сървъра, с който ние
верифицраме, че притежаваме домейна. Това се проверява от letsencrypt след като го свършим. След
това, за да поискаме ключ трябв да подадем на letsencrypt нашия публичен ключ и да го помолим да ни
генерира сертификат, като се подпише със своя private ключ. След това всички останали заявки ги
крипитираме с нашия private ключ и letsencrypt ги верифицира, използвайки публичния ключ.

Мисля, че общо взето разбирам как работи acme. Забравих да спомена, че самите letsencrypt са
просто CA. Не се нещо повече. Иначе вече мога да продължа напред към разучаване на acme-tiny.

След разглеждане на гитхъб разбрах, че като първа стъпка трябв да си гинерирам account.key, който е
private ключ, използван от acme-tiny, за да ми се генерира account за разговори с letsencrypt. След
това трябва да създам csr(certificate signing request) и domain private key, които ще се използват
за създаване на crt файла, който ще ми върне letsencrypt. domain private key-а после се използва и
от nginx за декриптирне на съобщения крипитирани с публичния ключ. Видях и че е необходима
допълнителна конфигурация за nginx за endpoint-а /.well_known/acme-challenge, в който acme що добавя
файловете, които ще се използват от letsencrypt, за да си потвърди, че е мой домейна. Още се чудя
дали не искам multi-domain сертификат, ама ми се струва, че май е по-смислено да са два
single-domain, защото самите сайтове са различни. Това не знам дали прави логика само в моята глава,
ама се надявам, че не. Видях, че стандарта за SSL сертификати е да се използват sha256 хешове. Не
помнех дали вече не са остарели те, но явно още се използват, а и сертификата ще е валиден само за
месец, което предполагам ще означава, че не е проблем: 
https://security.stackexchange.com/questions/165559/why-would-i-choose-sha-256-over-sha-512-for-a-ssl-tls-certificate#:~:text=SHA%2D256%20claims%20128%2Dbit,%2D512%20claims%20256%2Dbit.&text=SHA%2D512%20is%20generally%20faster,sha256%20sha512%20on%20your%20computer.)
Тук са описани няколко плюсове на sha256, като главния е, че е просто по-кратък, което пести
bandwidth.

Понеже се усетих, че нямам достъп до моята машина по DNS промених малко zone конфигурацията. Преди
беше ns1.weber.openfmi.net адресът, който сочи към моята машина, а сега го направих на
weber.openfmi.net, за да мога да я достъпя, като го използвам. С dig си потвърдих, че работи и с
гугъл опитах да достъпя домейна и стигнах до nginx страницата по подразбиране, което ми показва, че
работи.

Добавих новия път /.well-known/acme-challenge/ в конфигурацията на nginx-а и го рестартирах. За да
си потвърдя, че работи го отоврих в бразуръза и всичко бачкаше. Добавих и един файл, за да видя, че
се отваря. Погледнах какво прави alias директвиата http://nginx.org/en/docs/http/ngx_http_core_module.html#alias
и видях, че просто подменя пътя. Чудех се дали това влияе на директорията, но явно влияе и
overwrite-ва root. 

Пуснах комадната за генериране на сертификат, но за жалост не сработи. Не знам защо. Каза, че не е
успяло да ми валидира сертификата. След малко гугълване намерих този линк:
https://github.com/diafygi/acme-tiny/issues/166, което е моята грешка . Разбрах, че може да ми 
липсва ca-certificates пакета, съдържащ сертификатите на well known CAs. Инсталирах го и грешката се
промени. Съответно получих 403, защото версията на acme-tiny не поддържа acme версия 2 и ми е
забранено да се регистрирам. Съответно трябва да направя backport-а на buster пакета. Използвах
наученото от 8мото домашно и пак следвах стъпките от този линк: https://wiki.debian.org/SimpleBackportCreation
Свалих с apt source пакета после инсталирах dependency-тата после направих тестов build-ване на
пакета с fakeroot, а накрая го build-нах с dpk-buildpkackage и го инсталирах. Пуснах пак acme-tiny
командата и този път успешно бях регистриран, но валидацията се провали. Не беше успешно добавянето
на файла за challenge-а. Пуснах пак командата и докато вървеше пусках ls като маняк и видях, че
файла, който трябва да се добави е добавен. Явно конфигурацията на nginx-а не работи, защото не
намира файла. Интересното е, че ако дабавя файл на ръка работи. Не знам защо acme-tiny се проваля. 
Добавих техния сложен файл на ръка и достъпих пътя, към който е пратена заявката, но пак получих
отговор. Не ми е ясно, защо това не се получава и с acme-tiny. Получавам реално no route to host,
което не знам дали не значи, че като цяло не ми намера dns entry-то. Казва ми, че записва файла, но
не може да го свали. Не разбирам как се случва това, когато при мен си работи. Помислих си, че може
да е от машината, ама на един колега му пратих линка и успя да си свали файла. Еми не знам защо
става това. Ами сега като направих wget -4
http://november.fmi.fail/.well-known/acme-challenge/YSKbkEXxmZVG36qcpUaYU13F4QHcSZzxeEzoCvBV6qk
видях, че и на мен ми дава no route to host. Обаче интересното е, че се връща друг ip адрес, който
не съвпада с този на моята машина и това мега много ме обърква. Получавам 185.117.82.98, а
последното число трябва да е 118. Ако пусна wget от друга машина няма проблеми, но ако го пусна от
сървъра ip-то е различно и ми дава destination host unreachable. Нещо тука dns resolving-а не работи
много много. Разбрах, че горния ip address е сложен в /etc/hosts, като mapping за 185.117.82.98. Не
знам сега това нарочно ли е и дали, ако го махна няма да счупя нещо или е просто оставено, като
карта капан. Все пак съм наследил от някого тази машина уж XD. Страх ме е да не си счупя машината,
макар че не виждам защо това би я счупило, ама знам ли. Мисля да изтрия реда и да видя дали всичко
ще е ок тогава, ама не съм сигурен защо би бил там. Реших да попитам Ирина дали и при нея го има
това entry, ама вероятно го има, защото аз съм убеден, че не съм го добавял. Добре престраших се да
го закоментирам и стана. 

Сега обаче имам друга грешка. acme-tiny стига до host-а, но не намира файла, а и като го гледам май
не го слага в директорията или поне да правя ls през 2 секунди не показа нов файл да се слага и
после не го намира заради това и получавам 404. wget към пътя работи. Почти съм сигурен, че
acme-tiny просто не добавя файла, който иска да свали. Влезнах във файла на библиотаката под 
/usr/lib/python3/dist-packages/acme_tiny.py и добавих няколко логове, за да видя дали получавам
информация и дали се записва във файл. Видях, че наситина получавам дългия хеш за името на файла или
съдържанието. Не съм на 100%, кое и сега ще видя дали файла реално се създава, като му забраня да го
трие. Добре явно създава файла. Видях го вече в директорията, но нещо не може да го свали. Аз
успявам обаче с wget да го сваля използвайки същия път, който те използват. Не знам защо не работи.
Хипотезата ми е, че може да избързва и още преди nginx да е засякъл новия файл да се опитва да го
достъпи, ама това ми изглежда малко вероятно. Все пак ще пробвам с един sleep. Еми както очаквах не
е това. Не знам каква е причината заявката да не работи, като аз ако я правя с wget минава. И от
сървъра и от локалната ми машина. НАЙ–НАКРАЯ!!!!! Подкарах го и получих сертификат. Проблемът беше,
че бях оставил сървърът ми да слуша само на ipv4, а адреса се resolve-ваше и до ipv6, което
създаваше проблемите за генерирането на сертификата. Това го разбрах чак когато направих
--disable-check, което пропуска локалната проверка дали всичко работи и прави само глобалната.
Предполагам с --disable-check мога и цялостно да избегна проблемите, които имах по-горе с /etc/hosts
и да върна старото entry. Вече обаче имам подписан сертификат :D. Остана да автоматизирам процеса за
този сертификат и weber и да го подкарам на nginx-а 

Като начало искам да пробвам да заредя сертификата за november и после да подкарам weber за http,
защото иначе няма как да потвърдя, че притежавам домейна. Добавих keepalive_timeout и session_cache
директивите с идея да оптимизирам ssl handshake-а и да го направя по-бърз. Сътоветно първото не
убива сесията с клиента веднага, така че няколко заявки могат да се обработят като част от една
сесия, а второто пази информация за сесията между worker процесите за даден период от време. Така
примерно, ако ме обслужи друг worker процес при повторна заявак той ще знае информацията за сесията
ми. Тези бяха препоръчани от линка, който беше в точка 3) от по-рано. Този: 
https://nginx.org/en/docs/http/configuring_https_servers.html#sni

Разместих някои конфигурации, които ще важат и за двата сървъра в nginx.conf, защото ще се наследят.
Това са sendfile, tcp_nopush, sendfile_max_chunk и подобни. Също ssl конфигурациите ги направих на
централно място. Само сертификатите са на нивото на сървъра. Аз ще използвам само TLSv1.2 като
протокол, защото v1.3 не се поддържа от моя nginx, а другите са depricated. За cipher_suites ще ги
оставя по подразбиране. Не разбирам достатъчно от тях, за да кажа кои биха били по-добри. Тук
намерих препоръчителни cipher_suites: 
https://medium.com/@mvuksano/how-to-properly-configure-your-nginx-for-tls-564651438fe0
Също така видях, че за да използвам diffie hellman ми трябват ssl_dhparams, които доколкото разбрах
от тук https://security.stackexchange.com/questions/94390/whats-the-purpose-of-dh-parameters са
базата и простото число, над което правим mod. Там са описани и други оптимизации, като OCSP и HSTS.
Първото пести време на клиента като прикача потвърждение от CA-я, че сертификата, който му се е
подал не е изтекъл, а второто казва, че ако ще говориш с моя сървър ти трябва https. Цялостно ми се
струват ненужно поне за момента. Да по-бързо е, ама нека и без тях го подкарам. Dhparams все пак ще
го добавя, защото ми изглежда важно да имам големи и safe числа за diffie hellman. Също така мисля,
че файлът съдържа предварително изчислени a^b mod p с идеята по-бърза работа. Видях между друото, че
почти всички оцпии, които бях конфигурирал в november.conf така или иначе си ги е имало в
nginx.conf. Визирам по стандартните като sendfile и tcp_nodelay. Така разбрах, че не е имало смисъл
да ги пускам и щеше да е по-добре по-подробно да огледма какво имам, ама така или иначе трябваше да
прочета за какво са, така че няма някаква загуба. 

След всичките тези конфигурации като по магия https-а вече работи. Поне на november. За weber има
време, а и трябва да конфигурирам автоматично генериране на сертификати.

Като следваща стъпка реших да създам втори сървър за weber. Направих базовите конфигурации и
направих и redirect-а от http към https, който видях от тук: 
https://serverfault.com/questions/67316/in-nginx-how-can-i-rewrite-all-http-requests-to-https-while-maintaining-sub-dom
Съответното решние беше готино, защото имаше и линк към тази статия: 
https://www.nginx.com/resources/wiki/start/topics/tutorials/config_pitfalls/#taxing-rewrites , в
която са описани някои добри практики за работа с nginx. Прочетох само секцията с rewrite-те като
начало, но и в другите секции са обяснени неща като как да избягваме повтаряемост на код и примерно,
че е лоша идея на 50 места да имам index директива, което май съм направил. Опа XD.

Опитах се да се вържа към weber, който туко що наконфигурирах обаче като резултат получавах
страниците на november. Това не знам на какъв принцип е и доста ме дразни. Видях, че нещо съм
объркал symlink-овете за weber и затова не работи. Сега като http страница се отвори.

Установих, че пренасочването работи, но нищо не се зарежда като съдържание за https. Понеже още
нямам сертификат не знам дали не трябва да подкарам сървъра за http после да генерирам сертификат и
накрая да го пусна и за https. Според този thread ми трябва сертификат:
https://serverfault.com/questions/842779/set-nginx-https-on-port-443-without-the-certificate
Цялостно съм склонен да му повярвам иначе би било странно да работи. На гитхъба за acme_tiny пишеше,
че не е проблем да използвам http към https redirect за потвърждение на пътя, но това вероятно е,
ако имаш вече генериран сертификат, а не като цяло. За целта сега ще setup-на weber за http и ще си
генерирам сертификат, а после ще го направя за https само. При генериране на нов сертификат
redirect-а ще работи и само трябва да се подмени стария. 

За моя жалост не стана от раз генерирането на сертификата. Пак правеше проблеми с това, че няма
валиден път до моя сървър, което е странно. Направих dig да видя всичко ли е ок и пуснах наново
acme_tiny командата и този път сработи. Супер. Време да конфигурирам https-а. Подкарах
пренасочването и работи като гледам на https. Остана само да видя дали, ако едновременно работят
weber и november ще има проблеми. Дано не. Учудващо работят и всичко е супер. Единствено се чудя за
горните SNI неща, които разгледах за какъв случай са. Ами SNI май по подразбиране се използва от
nginx, а не се нуждае просто от повече конфигурация, както си мислех по-рано иначе нямам обяснение.
Точно в случая, който е описан тук попадам: http://nginx.org/en/docs/http/configuring_https_servers.html
Няколко сървъра искат да използват https с един ip address. Моето ip е това на машината. Освен, ако
едното не слуша на ipv6 адреса, а другото на ipv4, ама в моя случай слушам и на двете и за двата
сървъра. Ами не можах да намера нещо, което да потвърждава това, но смятам, че е така иначе няма
логика.

Опитах се да генерирам наново weber сертификата, като сега вече има друг сертификат, който работи,
т. е. докато https работи исках да видя дали мога да си генерирам успешно сертификат, но ми върна
това: urlopen error [Errno -5] No address associated with hostname . Това не знам какво ще рече,
имайки предвид, че преди малко успешно си генерирах сертификат. Като скипнах верификацията мина.
Може би redirect-а бугва python кода или не знам. Общо взето request-а се проваля остатъка от кода
работи. Не знам дали е заради redirect-а или по друга причина, ама с wget или през браузъра успешно
минава всичко. Не знам дали има смисъл да мъча още това, ама ме дразни. Като направих dig отново пак
мина. Нещо hostname resolution-а не работи, като хората за тази команда, ама не знам защо. Ааааах
разбрах защо. Ирина не си е оправила конфигурацията на нейната машина откакто се опитвахме да
направим това с ключовете и сега, ако ме нацели мен минава, но ако нацели нейната машина се чупи.
Трябва да я помоля да го оправи това. Иначе успешно се генерира един от пътитите, в които опитах,
така че май е ок да мина към автоматизацията.

Преди това се сетих, че трябва конфигурацията ми да се одобри от sslabs:
https://www.ssllabs.com/ssltest/analyze.html?d=weber.openfmi.net и затова го пуснах на weber. Оказа
се, че key exchange-а и cipher strength-а ми не са топ. Тях ще трябва да ги сменя или да видя как да
ги подобря. Иначе имам A към момента. Проверих в интернет как да изкарам A+ и общо взето HSTS трябва
да пусна. Ето тук го видях: 
https://www.namecheap.com/support/knowledgebase/article.aspx/9752/38/how-do-i-get-a-rating-in-ssllabs/
То се пуска като се добави Strict Transport Security header, което казва на бразуъра ми, че като
говори с моя сървър е препоръчително да използва https. От уикипедия научих, че max-age параметъра
казва колко време трябва да се използва https за говорене със сървъра. Съответно мога да кажа за
следващата година искам да говорих https на сървъра. Това общо взето казва, че ако сигурността на
връзката не може да се осигури то връзката трябва да се терминира. Като гледам не е съвсем така,
защото успешно се връзвам и с https и с http към november.fmi.fail без браузъра ми да минава на
https. HSTS според уикипедия е въведено във време, когато не е било ясно дали даден сървър може да
използва https. Тогава, ако видиш, че на сайта връзката е с http е доста вероятно да предположиш, че
просто още няма поддръжка. Това, че се използва http обаче може и да е характерно за man in the
middle атака, в която https връзката се конвертира до http. Понеже това не е било ясно в кой случай
сме, хората са добавил съответния header, за да може сървъра да си кажа, ако поддръжа https и да се
знае дали няма атака. Аз добавих  header-а за целия сървър между другото и затова по-рано споменах,
че е странно, че не минавам от http на https за november.

Цялостно сега изкарах A+ и за weber и за november, така че наистина остана само автоматизацията,
която вероятно ще е просто един cronjob. Чудя се дали сертификатите да ги оставя където са. В
момента стоят под nginx директорията в certificates и там има отделни файлове за ключове и
сертификати, csr-и и тн. Може би е по-добре да са в отделна директория, ама не съм сигурен. Иначе
остава да напиша един кратък баш скрипт и да пусна един cronjob.

Започнах с автоматизацията и общо взето тя се свежда до две acme-tiny команди за weber и november,
които подменят crt файловете и команда, която презарежда конфигурацията на nginx. Преместих
файловете със сертификатите под /etc/certs. Струваше ми се по-логично да си имат отделна директория
пък не знам. Реших да проверя дали е лоша идея да използвам един и същ CSR при всяко генериране на
сертификат и отговорът беше да според този сайт:
https://www.appviewx.com/education-center/certificate-renewal-and-revocation/do-i-have-to-generate-a-new-csr-to-get-my-certificate-renewed/#:~:text=Though%20some%20web%20servers%20may,used%20in%20the%20old%20one.
Причината е, че се добавят нови методи за криптиране със новите CSR-и. Ключа обаче може спокойно да
си е един и същ, така че добавих замяна и на csr файловете за november и weber. 

Добре в процеса на оправянето на script-та генерирах доста сертификати за november и общо взето
триех старите и за моя жалост това доведе до удряне на rate-limit за домейна. С други думи в момента
нямам сертификат за november и не знам как да генерирам такъв. Мислех да видя дали мога да revoke-на
сертификатите, но не мога без да имам достъп до тях. Това не беше добре обмислено :(.

Имах голям късмет. Браузърът ми още пазеше сертификата за november и аз го експортирах и го копирах
на сървъра. Вече цялостно всичко пак работи. Ще спра да генерирам сертификати като маниак. Написах
скрипта, така че да се праща съобщение до root, ако не се изпълни успешно.

Добавих и cronjob в crontab, че на всеки два месеца трябва да се пуска скрипта ми. Лошото е, че заради 500-те 
сертификата, които направих нямам как да мине докато не минат поне 3 месеца и не изтекат настоящите сертификати,
за да мога да генерирам нов. Не знам дали дори тогава ще мога, защото може да се налага да ги revoke-на, което
не мога с acme-tiny. По принцип би било по-добре да се revoke-не настоящия ми сертификат и да се генерира нов.
Така би сработило, ама да. С acme_tiny не може заради размера на файла. Искат да е до 200 реда и затова не искат 
да добавят друга функционалност.

Цялостно мисля, че съм готов с общата задача. С малко късмет всикчо все още работи :D. Дано си остане така.
Само cronjob-а е леко съмнителен, ама вярвам, че е ок. И за в бъдеще не трябва да си трия сертификатите. 
Направих го така, че преди да се пусне скрипта се прави backup на сертификатите. Добра идея е, ако скрипта
не мине да не се пуска пак, като идиот, защото може пак да се затрият сертификатите, защото cp командата,
която използвам би презаписала backup-а от първото пускане. Цялостно май е по-смислено да се направи
отделен cronjob, който на някакъв период да прави backup-и и после да ги затрива, ако са твърде много.
Това не ми се струва, като важна част от задачата, а и имам друга работа, така че няма да го мъча.

CGI, FCGI, uwsgi, wsgi:
От тук разбрах, че разликата между CGI и fastgi, е че fastgi не създава отделни процеси за всяка заявка, 
а е само един процес: https://medium.com/learn-backend/cgi-fastcgi-scgi-and-wsgi-1dd94ff5e779

WSGI: Gateway interface, който използва fastcgi за разговори с python сървъри: 
https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface

uwsgi: https://uwsgi-docs.readthedocs.io/en/latest/ - това му е документацията. Доколкото разбирам нямао
общо със wsgi освен, че това е първия plugin за uwsgi. Идеята на uwsgi е да даде едно api за комуникация 
със всякакви application сървъри, process manager-и и системи за мониториране. Също така може лесно да се
extend-ва с нови plugin-и. Струва ми се, че е просто по-комплексна версия на fastcgi. Самия fastcgi е протокол,
така че това може да е нещо като имплементация. Не съм съвсем сигурен. В интернет няма много сравнения между
двете, което ме навежда на идеята, че вероятно са с различни идеи, ама пак не съм сигурен. С още гугълване
не открих много. Тук пише, че uWSGI може да използва fastcgi или uwsgi(малки букви): 
https://uwsgi-docs.readthedocs.io/en/latest/WSGIquickstart.html 
Това ме накара да потърся за uwsgi протокола, но открих само това: 
https://uwsgi-docs.readthedocs.io/en/latest/Protocol.html
Като гледам какво излиза в гугъл за него това е по-скоро алтернатива на http. АААААААААААААА. Аз съм тъп.
Точно това е. Това вероятно използва web socket-и и е web socket gateway interface(wsgi). Офф сцепих.
Затова не се сравянва с fascgi, защото идеите са, че едното е за http, а другото за web socket-и. Ок 
разбрах.

SCGI: https://en.wikipedia.org/wiki/Simple_Common_Gateway_Interface Доколкото разбирам е просто FSCGI, но 
позволява по-лесно parse-ване на заявки. Не разбирам точно как го прави. Пише, че се възползва от това, че 
http заявката вече е parse-ната по някакъв начин, ама не ми се задълбава в момента точно как го прави. 

Стана ми интересно защо файлове с .ht ги забраняваме и гугълнах и видях това:
https://httpd.apache.org/docs/current/howto/htaccess.html Явно има файлове за apache, които контролират
конфигурации за директории. Не знам дали това е идеята и ще правим после нещо с apache-а или не, ама ще
видим. 

Време: 23 часа

Днеска реших да видя нещо на сървъра и се усетих, че не бях направил ssl тестовете за ipv6 или не ги бях
погледнал. Пуснах пак всичко и мина и на weber имах A+. На november обаче имах B и за ipv4 и за ipv6. Това
вероятно е заради сертификата, който взех от браузъра или по-точно пише непълен сертификат, т.е. е доста
вероятно да е точно заради това. Погледнах и rate limit-ите на letsencrypt за сертификати и май мога да 
правя пет duplicate сертификата на седмица, т. е. следващата седмица ще гледам това да го оправя. Мога
да renewel-на сертификат, ама и това е изисква да сваля друг acme клиент. Странното е, че преди и november
беше A+ уж със подменения сертификат. Абе каквото. Ще се опитам да го сменя когато мога. Мисля, че това
е грешката. Надявам се да не е голям проблем. 
